{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "from models.Regression_models import (get_thz_model, \n",
    "    get_loss_function, \n",
    "    advanced_train_model,\n",
    "    evaluate_model, AdaptivePrecisionLoss)\n",
    "from models.utils import identify_device, display_model\n",
    "from models.regression_utils import get_train_val_loaders, denormalize_material_params\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = identify_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"regression_data/train_3_layer_512_nonoise_n1to8.pt\"\n",
    "\n",
    "train_loader, val_loader, num_samples = get_train_val_loaders(\n",
    "    dataset_path=file_path,\n",
    "    batch_size=128,\n",
    "    val_split=0.1\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ultra model on 32 samples\n",
    "model_type = 'ultra'  # Much larger model\n",
    "model = get_thz_model(model_type)\n",
    "loss_type = 'weighted'\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Ultra model parameters: {total_params:,}\")  # Should be ~10M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Testing Overfitting Capability ===\")\n",
    "\n",
    "# Create small dataset for overfitting test\n",
    "small_dataset = []\n",
    "count = 0\n",
    "for batch in train_loader:\n",
    "    for i in range(len(batch[0])):\n",
    "        # Keep correct shape: [channels, length]\n",
    "        small_dataset.append((batch[0][i], batch[1][i]))\n",
    "        count += 1\n",
    "        if count >= 32:\n",
    "            break\n",
    "    if count >= 32:\n",
    "        break\n",
    "\n",
    "# Create small dataloader\n",
    "small_loader = DataLoader(small_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Test model on small dataset\n",
    "test_model = get_thz_model(model_type)\n",
    "test_loss_fn = get_loss_function(loss_type).to(device)\n",
    "\n",
    "# Test overfitting with higher learning rate\n",
    "test_model, test_history = advanced_train_model(\n",
    "    model=model,\n",
    "    train_loader=small_loader,  # 32 samples\n",
    "    num_epochs=200,\n",
    "    initial_lr=5e-3,  # Higher LR\n",
    "    loss_fn=AdaptivePrecisionLoss(),  # New adaptive loss\n",
    "    patience=300,\n",
    "    save_best=False\n",
    ")\n",
    "\n",
    "print(f\"Final training loss on 32 samples: {test_history['train_loss'][-1]:.6f}\")\n",
    "if test_history['train_loss'][-1] < 0.001:\n",
    "    print(\"✅ Model can overfit! Architecture is capable.\")\n",
    "else:\n",
    "    print(\"⚠️  Model struggling to overfit. May need architecture changes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, pred_denorm, target_denorm = evaluate_model(test_model, small_loader, denormalize_material_params, device=device)\n",
    "\n",
    "len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Starting Full Training ===\")\n",
    "\n",
    "# Fresh model for full training\n",
    "model = get_thz_model(model_type, input_channels=1, output_dim=9)\n",
    "loss_fn = get_loss_function(loss_type).to(device)\n",
    "\n",
    "# Train the model\n",
    "trained_model, history = advanced_train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=500,  # Much longer training\n",
    "    initial_lr=1e-3,\n",
    "    loss_fn=loss_fn,\n",
    "    denormalize_fn=denormalize_material_params,  # For real-world metrics\n",
    "    device=device,\n",
    "    patience=50,    # Early stopping patience\n",
    "    min_delta=1e-6, # Minimum improvement threshold\n",
    "    save_best=True,\n",
    "    model_save_path=\"best_thz_advanced_model.pt\"\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(history['train_loss'], label='Training Loss', alpha=0.8)\n",
    "ax1.plot(history['val_loss'], label='Validation Loss', alpha=0.8)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')  # Log scale for better visualization\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "ax2.plot(history['learning_rate'], color='orange')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.set_title('Learning Rate Schedule')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best validation loss: {min(history['val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Comprehensive Evaluation ===\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "metrics, pred_denorm, target_denorm = evaluate_model(\n",
    "    trained_model, val_loader, denormalize_material_params, device\n",
    ")\n",
    "\n",
    "# Create metrics table\n",
    "table_data = []\n",
    "target_accuracy = {\n",
    "    'n1': 0.01, 'n2': 0.01, 'n3': 0.01,\n",
    "    'k1': 1e-5, 'k2': 1e-5, 'k3': 1e-5,\n",
    "    'd1': 1e-6, 'd2': 1e-6, 'd3': 1e-6\n",
    "}\n",
    "\n",
    "for param, metric in metrics.items():\n",
    "    target_acc = target_accuracy[param]\n",
    "    rmse_ratio = metric['rmse'] / target_acc\n",
    "    meets_target = \"✅\" if metric['rmse'] <= target_acc else \"❌\"\n",
    "    \n",
    "    table_data.append([\n",
    "        param,\n",
    "        f\"{metric['rmse']:.2e}\",\n",
    "        f\"{metric['mae']:.2e}\",\n",
    "        f\"{metric['r2']:.4f}\",\n",
    "        f\"{metric['max_error']:.2e}\",\n",
    "        f\"{target_acc:.0e}\",\n",
    "        f\"{rmse_ratio:.1f}x\",\n",
    "        meets_target\n",
    "    ])\n",
    "\n",
    "headers = ['Param', 'RMSE', 'MAE', 'R²', 'Max Error', 'Target', 'Ratio', 'Meets Target']\n",
    "print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "# Overall summary\n",
    "total_meeting_target = sum(1 for param, metric in metrics.items() \n",
    "                          if metric['rmse'] <= target_accuracy[param])\n",
    "print(f\"\\nParameters meeting target accuracy: {total_meeting_target}/9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of predictions for visualization\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(val_loader))\n",
    "    inputs, targets = sample_batch\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    predictions = trained_model(inputs)\n",
    "    \n",
    "    # Denormalize for plotting\n",
    "    pred_denorm_batch = denormalize_material_params(predictions.cpu())\n",
    "    target_denorm_batch = denormalize_material_params(targets.cpu())\n",
    "\n",
    "# Plot predictions vs targets for first few samples\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "param_names = ['n1', 'k1', 'd1', 'n2', 'k2', 'd2', 'n3', 'k3', 'd3']\n",
    "\n",
    "for i, (ax, param) in enumerate(zip(axes.flat, param_names)):\n",
    "    pred_vals = pred_denorm_batch[:10, i].numpy()  # First 10 samples\n",
    "    target_vals = target_denorm_batch[:10, i].numpy()\n",
    "    \n",
    "    ax.scatter(target_vals, pred_vals, alpha=0.7)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val, max_val = min(target_vals.min(), pred_vals.min()), max(target_vals.max(), pred_vals.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect')\n",
    "    \n",
    "    ax.set_xlabel(f'True {param}')\n",
    "    ax.set_ylabel(f'Predicted {param}')\n",
    "    ax.set_title(f'{param} (R² = {metrics[param][\"r2\"]:.3f})')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "torch.save(history, 'training_history.pt')\n",
    "\n",
    "# Save final metrics\n",
    "metrics_serializable = {}\n",
    "for param, metric in metrics.items():\n",
    "    metrics_serializable[param] = {k: float(v) for k, v in metric.items()}\n",
    "\n",
    "with open('final_metrics.json', 'w') as f:\n",
    "    json.dump(metrics_serializable, f, indent=2)\n",
    "\n",
    "print(\"Results saved!\")\n",
    "print(f\"Model saved as: best_thz_advanced_model.pt\")\n",
    "print(f\"Training history saved as: training_history.pt\") \n",
    "print(f\"Metrics saved as: final_metrics.json\")\n",
    "\n",
    "# Model summary\n",
    "print(f\"\\n=== Final Model Summary ===\")\n",
    "print(f\"Model type: {model_type}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Loss function: {loss_type}\")\n",
    "print(f\"Best validation loss: {min(history['val_loss']):.6f}\")\n",
    "print(f\"Parameters meeting target: {total_meeting_target}/9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If results aren't good enough, try these variations:\n",
    "\n",
    "print(\"\\n=== Alternative Configurations to Try ===\")\n",
    "print(\"1. MultiHead model:\")\n",
    "print(\"   model = get_thz_model('multihead')\")\n",
    "print(\"   loss_fn = get_loss_function('multitask')\")\n",
    "\n",
    "print(\"\\n2. Higher learning rate:\")\n",
    "print(\"   initial_lr=5e-3\")\n",
    "\n",
    "print(\"\\n3. Longer training:\")\n",
    "print(\"   num_epochs=1000, patience=100\")\n",
    "\n",
    "print(\"\\n4. Different loss weighting:\")\n",
    "print(\"   Edit WeightedParameterLoss weights in regression_models.py\")\n",
    "\n",
    "print(\"\\nIf overfitting test failed, the architecture needs to be even larger!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
